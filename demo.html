<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0" />
	<title>Degla — Demo</title>
	<script src="https://cdn.tailwindcss.com"></script>
	<style>
		@keyframes pulseDot { 0%, 100% { opacity: .25; } 50% { opacity: 1; } }
		.pulse-dot { animation: pulseDot 1.2s ease-in-out infinite; }
	</style>
</head>
<body class="min-h-screen bg-gradient-to-b from-slate-950 via-slate-900 to-slate-950 text-white">
	<div class="max-w-5xl mx-auto px-6 py-10">
		<header class="flex items-center justify-between gap-4">
			<a href="./index.html" class="flex items-center gap-3">
				<div class="w-10 h-10 rounded-xl bg-white/10 ring-1 ring-white/10 flex items-center justify-center font-semibold">D</div>
				<div class="leading-tight">
					<div class="font-semibold">Try Now</div>
					<div class="text-xs text-slate-400">Chat + low‑latency voice (Cartesia)</div>
				</div>
			</a>

			<div class="flex items-center gap-2">
				<button id="videoBtn" class="px-3 py-2 rounded-lg ring-1 ring-white/10 hover:bg-white/10 text-sm">
					Video call
				</button>
				<button id="phoneBtn" class="px-3 py-2 rounded-lg bg-emerald-600 hover:bg-emerald-500 text-sm font-medium">
					Phone call
				</button>
			</div>
		</header>

		<main class="mt-8 grid grid-cols-1 lg:grid-cols-[340px_1fr] gap-6">
			<section class="rounded-2xl bg-white/5 ring-1 ring-white/10 p-5">
				<div class="flex items-center gap-4">
					<img id="expertAvatar" class="w-16 h-16 rounded-2xl object-cover ring-1 ring-white/10"
							 alt="Expert avatar"
							 src="https://yt3.googleusercontent.com/ytc/AIdro_nDvyq2NoPL626bk1IbxQ94SfQsD-B0qgZchghtQNkLWoEz=s900-c-k-c0x00ffffff-no-rj"/>
					<div class="min-w-0">
						<div class="flex items-center gap-2">
							<h2 id="expertName" class="font-semibold truncate">Andrej Karpathy</h2>
							<span class="text-[11px] text-slate-300 px-2 py-0.5 rounded-full bg-white/10 ring-1 ring-white/10">Expert</span>
						</div>
						<p id="expertHeadline" class="text-sm text-slate-300 truncate">Deep Learning • LLMs</p>
					</div>
				</div>

				<div class="mt-4 text-sm text-slate-300 leading-relaxed">
					<p id="expertBio" class="text-slate-300">
						Ask Andrej about deep learning, training LLMs, evals, deployment, and practical engineering tradeoffs.
					</p>
				</div>

				<div class="mt-4 rounded-xl bg-black/30 ring-1 ring-white/10 p-4">
					<div class="flex items-center justify-between">
						<div class="text-xs text-slate-400">Voice status</div>
						<div id="voiceDot" class="w-2.5 h-2.5 rounded-full bg-slate-500"></div>
					</div>
					<div id="voiceStatus" class="mt-2 text-sm">Idle</div>
					<div class="mt-2 text-xs text-slate-400">
						Cartesia voice id:
						<code class="text-slate-200">27f0940b-53d1-4612-9724-9ae88f855257</code>
					</div>
				</div>

				<div class="mt-4 text-xs text-slate-400">
					Tip: after the first Andrej reply, you can keep chatting naturally—context is preserved here.
				</div>
			</section>

			<section class="rounded-2xl bg-white/5 ring-1 ring-white/10 overflow-hidden flex flex-col min-h-[70vh]">
				<div class="px-5 py-4 border-b border-white/10 flex items-center justify-between gap-3">
					<div class="flex items-center gap-3 min-w-0">
						<img class="w-9 h-9 rounded-xl object-cover ring-1 ring-white/10" id="chatAvatar"
								 alt="Andrej"
								 src="https://yt3.googleusercontent.com/ytc/AIdro_nDvyq2NoPL626bk1IbxQ94SfQsD-B0qgZchghtQNkLWoEz=s900-c-k-c0x00ffffff-no-rj"/>
						<div class="min-w-0">
							<div class="font-medium truncate">Andrej Karpathy</div>
							<div id="subStatus" class="text-xs text-slate-400">Ready</div>
						</div>
					</div>
					<div class="text-xs text-slate-400">Try Now • Web</div>
				</div>

				<div id="messages" class="flex-1 overflow-y-auto px-5 py-4 space-y-3">
					<!-- messages go here -->
				</div>

				<div class="border-t border-white/10 p-4">
					<form id="composer" class="flex items-end gap-3">
						<div class="flex-1">
							<textarea id="input"
								class="w-full resize-none rounded-xl bg-black/30 ring-1 ring-white/10 px-4 py-3 text-sm outline-none focus:ring-2 focus:ring-emerald-500/60"
								rows="1"
								placeholder="Ask Andrej… (Shift+Enter for newline)"></textarea>
							<div class="mt-2 flex items-center justify-between">
								<div class="text-xs text-slate-500">Enter to send • Shift+Enter newline</div>
								<div id="sendHint" class="text-xs text-slate-500"></div>
							</div>
						</div>
						<button id="sendBtn" type="submit"
							class="px-4 py-3 rounded-xl bg-indigo-600 hover:bg-indigo-500 font-medium text-sm disabled:opacity-40 disabled:cursor-not-allowed">
							Send
						</button>
					</form>
				</div>
			</section>
		</main>
	</div>

	<!-- Video Call Modal -->
	<div id="videoModal" class="fixed inset-0 z-50 hidden">
		<div class="absolute inset-0 bg-black/70 backdrop-blur-sm"></div>
		<div class="relative max-w-5xl mx-auto px-6 py-10 h-full flex flex-col">
			<div class="flex items-center justify-between gap-3">
				<div class="flex items-center gap-3 min-w-0">
					<img id="videoAvatar" class="w-10 h-10 rounded-xl object-cover ring-1 ring-white/10"
							 alt="Avatar"
							 src="https://yt3.googleusercontent.com/ytc/AIdro_nDvyq2NoPL626bk1IbxQ94SfQsD-B0qgZchghtQNkLWoEz=s900-c-k-c0x00ffffff-no-rj"/>
					<div class="min-w-0">
						<div class="font-semibold truncate">Video Call • <span id="videoCalleeName">Andrej Karpathy</span></div>
						<div id="videoStatus" class="text-xs text-slate-300">Idle</div>
					</div>
				</div>
				<div class="flex items-center gap-2">
					<button id="videoHangupBtn" class="px-3 py-2 rounded-lg bg-red-600 hover:bg-red-500 text-sm font-medium">
						Hang up
					</button>
					<button id="videoCloseBtn" class="px-3 py-2 rounded-lg ring-1 ring-white/10 hover:bg-white/10 text-sm">
						Close
					</button>
				</div>
			</div>

			<div class="mt-6 grid grid-cols-1 lg:grid-cols-[1fr_360px] gap-6 flex-1 min-h-0">
				<div class="rounded-2xl bg-black/30 ring-1 ring-white/10 overflow-hidden flex items-center justify-center min-h-[50vh]">
					<video id="heygenVideo" class="w-full h-full object-cover" autoplay playsinline></video>
					<div id="videoPlaceholder" class="absolute text-center px-8">
						<div class="text-lg font-semibold">Connecting…</div>
						<div class="text-sm text-slate-300 mt-2">Starting LiveAvatar stream.</div>
					</div>
				</div>

				<div class="rounded-2xl bg-white/5 ring-1 ring-white/10 p-5 flex flex-col gap-4 min-h-0">
					<div class="text-sm text-slate-200 font-medium">Controls</div>
					<div class="text-sm text-slate-300 leading-relaxed">
						Speak normally. In <b>FULL mode</b>, LiveAvatar handles listening (ASR), reasoning (LLM), and speaking (TTS) inside the LiveKit room.
					</div>
					<div class="rounded-xl bg-black/30 ring-1 ring-white/10 p-4">
						<div class="text-xs text-slate-400">Live transcript</div>
						<div id="videoTranscript" class="mt-2 text-xs text-slate-200 whitespace-pre-wrap break-words">—</div>
					</div>
					<div class="rounded-xl bg-black/30 ring-1 ring-white/10 p-4">
						<div class="flex items-center justify-between">
							<div class="text-xs text-slate-400">Mic level</div>
							<div id="videoMicText" class="text-[11px] text-slate-400">—</div>
						</div>
						<div class="mt-2 h-2 rounded bg-white/10 overflow-hidden">
							<div id="videoMicBar" class="h-2 w-0 bg-emerald-500"></div>
						</div>
					</div>
					<div class="rounded-xl bg-black/30 ring-1 ring-white/10 p-4">
						<div class="text-xs text-slate-400">Tip</div>
						<div class="mt-2 text-sm">If the video is blank, click anywhere once (browser autoplay policy).</div>
					</div>
					<div class="rounded-xl bg-black/30 ring-1 ring-white/10 p-4">
						<div class="text-xs text-slate-400">Status</div>
						<div id="videoDebug" class="mt-2 text-xs text-slate-300 break-words">—</div>
					</div>
					<div class="mt-auto text-[11px] text-slate-500">
						Live streaming uses LiveAvatar credits. Sessions should be stopped when not in use.
					</div>
				</div>
			</div>
		</div>
	</div>

	<!-- LiveKit client (used to join LiveAvatar LiveKit rooms) -->
	<script type="module">
		// We load LiveKit client in-module and expose it globally so the existing script (non-module) can use it.
		try {
			const mod = await import('https://esm.sh/livekit-client@2.7.5');
			window.__LIVEKIT__ = mod;
		} catch (e) {
			console.warn('[demo] Failed to load LiveKit client', e);
			window.__LIVEKIT__ = null;
		}
	</script>

	<script>
		// ---- Config ----
		const API_BASE = ''; // same-origin
		const ENDPOINTS = {
			realtimeSession: `${API_BASE}/ucm/realtime_session.php`,
			cartesiaTts: `${API_BASE}/ucm/cartesia_tts.php`,
			experts: `${API_BASE}/ucm/experts.php`,
			demoChat: `${API_BASE}/web/demo_chat.php`,
			realtimeSdpProxy: `${API_BASE}/web/realtime_sdp.php`,
			liveavatarCreate: `${API_BASE}/web/liveavatar_session_create.php`,
			liveavatarStart: `${API_BASE}/web/liveavatar_session_start.php`,
			liveavatarStop: `${API_BASE}/web/liveavatar_session_stop.php`,
		};
		const OPENAI_REALTIME_MODEL = 'gpt-4o-realtime-preview-2024-12-17';
		const OPENAI_CHAT_MODEL = 'gpt-4o-mini';
		const DEFAULT_CARTESIA_VOICE_ID = '27f0940b-53d1-4612-9724-9ae88f855257';

		// These endpoints only require presence; they don't validate.
		const DEMO_USER_ID = 'demo';
		const DEMO_PASS = 'demo';

		// ---- UI helpers ----
		const el = (id) => document.getElementById(id);
		const messagesEl = el('messages');
		const inputEl = el('input');
		const sendBtn = el('sendBtn');
		const phoneBtn = el('phoneBtn');
		const videoBtn = el('videoBtn');
		const voiceStatusEl = el('voiceStatus');
		const voiceDotEl = el('voiceDot');
		const subStatusEl = el('subStatus');

		// Video modal elements
		const videoModalEl = el('videoModal');
		const videoCloseBtn = el('videoCloseBtn');
		const videoHangupBtn = el('videoHangupBtn');
		const heygenVideoEl = el('heygenVideo');
		const videoPlaceholderEl = el('videoPlaceholder');
		const videoStatusEl = el('videoStatus');
		const videoDebugEl = el('videoDebug');
		const videoTranscriptEl = el('videoTranscript');
		const videoMicBarEl = el('videoMicBar');
		const videoMicTextEl = el('videoMicText');
		const videoCalleeNameEl = el('videoCalleeName');
		const videoAvatarEl = el('videoAvatar');

		const setVoiceStatus = (text, state = 'idle') => {
			voiceStatusEl.textContent = text;
			const map = {
				idle: 'bg-slate-500',
				active: 'bg-emerald-500',
				busy: 'bg-amber-400',
				error: 'bg-red-500',
			};
			voiceDotEl.className = `w-2.5 h-2.5 rounded-full ${map[state] || map.idle} ${state === 'busy' ? 'pulse-dot' : ''}`;
		};

		const addMessage = ({ role, content, streaming = false }) => {
			const isUser = role === 'user';
			const wrap = document.createElement('div');
			wrap.className = `flex ${isUser ? 'justify-end' : 'justify-start'}`;

			const bubble = document.createElement('div');
			bubble.className =
				`max-w-[85%] rounded-2xl px-4 py-3 text-sm leading-relaxed ring-1 ring-white/10 ` +
				(isUser ? 'bg-indigo-600/40' : 'bg-black/30');
			bubble.textContent = content;
			if (streaming) bubble.dataset.streaming = '1';

			wrap.appendChild(bubble);
			messagesEl.appendChild(wrap);
			messagesEl.scrollTop = messagesEl.scrollHeight;
			return bubble;
		};

		const updateStreamingBubble = (bubble, nextText) => {
			if (!bubble) return;
			bubble.textContent = nextText;
			messagesEl.scrollTop = messagesEl.scrollHeight;
		};

		const autosizeTextarea = () => {
			inputEl.style.height = 'auto';
			inputEl.style.height = Math.min(inputEl.scrollHeight, 160) + 'px';
		};

		// ---- Expert loading (optional) ----
		let expert = {
			name: 'Andrej Karpathy',
			headline: 'Deep Learning • LLMs',
			bio: 'Ask Andrej about deep learning, training LLMs, evals, deployment, and practical engineering tradeoffs.',
			avatar_url: 'https://yt3.googleusercontent.com/ytc/AIdro_nDvyq2NoPL626bk1IbxQ94SfQsD-B0qgZchghtQNkLWoEz=s900-c-k-c0x00ffffff-no-rj',
			tags: ['deep learning', 'llms', 'engineering'],
			voice_id: '',
			avatar_id: '',
		};

		const renderExpert = () => {
			el('expertName').textContent = expert.name || 'Andrej Karpathy';
			el('expertHeadline').textContent = expert.headline || 'Deep Learning • LLMs';
			el('expertBio').textContent = expert.bio || '';
			const av = expert.avatar_url || expert.avatarUrl || '';
			if (av) {
				el('expertAvatar').src = av;
				el('chatAvatar').src = av;
				videoAvatarEl.src = av;
			}
			videoCalleeNameEl.textContent = expert.name || 'Andrej Karpathy';
		};

		const getSelectedExpertId = () => {
			try {
				const qs = new URLSearchParams(window.location.search || '');
				const raw = qs.get('expert_id') || '';
				const n = Number(raw);
				return Number.isFinite(n) && n > 0 ? n : null;
			} catch {
				return null;
			}
		};

		const loadExpertFromBackend = async () => {
			try {
				const selectedId = getSelectedExpertId();
				const url = selectedId
					? `${ENDPOINTS.experts}?id=${encodeURIComponent(String(selectedId))}`
					: `${ENDPOINTS.experts}?q=${encodeURIComponent('Andrej Karpathy')}`;
				const res = await fetch(url, { cache: 'no-store' });
				if (!res.ok) return;
				const json = await res.json();
				const row = Array.isArray(json) ? (json[0] || null) : json;
				if (!row) return;
				expert = {
					name: row.name || expert.name,
					headline: row.headline || expert.headline,
					bio: row.bio || expert.bio,
					avatar_url: row.avatar_url || expert.avatar_url,
					voice_id: row.voice_id || expert.voice_id || '',
					avatar_id: row.avatar_id || expert.avatar_id || '',
					tags: (() => {
						try { return row.tags ? JSON.parse(row.tags) : expert.tags; } catch { return expert.tags; }
					})(),
				};
				renderExpert();
			} catch {
				// ignore, fallback to default expert
			}
		};

		const buildSystemPrompt = () => {
			const tags = Array.isArray(expert.tags) ? expert.tags : [];
			const bio = (expert.bio || '').trim();
			return (
				`You are ${expert.name || 'Andrej Karpathy'}.\n` +
				`Use the persona below to answer in first person.\n\n` +
				`BIO:\n${bio}\n\n` +
				`TAGS:\n${JSON.stringify(tags)}\n\n` +
				`Rules:\n` +
				`- Be concise, practical, and high-signal.\n` +
				`- Prefer concrete advice, checklists, and tradeoffs.\n` +
				`- If context is missing, ask 1-2 clarifying questions.\n` +
				`- Output plain text only (no markdown). Never use asterisks like **, no bullet formatting, and no code fences.\n`
			);
		};

		// ---- OpenAI key retrieval (same endpoint as CallScreen) ----
		let openaiKey = null;
		const getOpenAIKey = async () => {
			if (openaiKey) return openaiKey;
			const body = new URLSearchParams({ user_id: DEMO_USER_ID, pass: DEMO_PASS }).toString();
			const res = await fetch(ENDPOINTS.realtimeSession, {
				method: 'POST',
				headers: { 'Content-Type': 'application/x-www-form-urlencoded' },
				body,
				cache: 'no-store',
			});
			const data = await res.json();
			if (!data?.success || !data.api_key) throw new Error(data?.error || 'Failed to load OpenAI key');
			openaiKey = data.api_key;
			return openaiKey;
		};

		// ---- Chat (streaming) ----
		const chatMessages = [
			{ role: 'system', content: buildSystemPrompt() },
		];

		const streamChatCompletion = async (userText) => {
			const key = await getOpenAIKey();

			chatMessages[0] = { role: 'system', content: buildSystemPrompt() };
			chatMessages.push({ role: 'user', content: userText });

			subStatusEl.textContent = 'Thinking…';
			sendBtn.disabled = true;

			const assistantBubble = addMessage({ role: 'assistant', content: '', streaming: true });
			let acc = '';

			let usedProxy = false;
			try {
				const res = await fetch('https://api.openai.com/v1/chat/completions', {
					method: 'POST',
					headers: {
						'Content-Type': 'application/json',
						'Authorization': `Bearer ${key}`,
					},
					body: JSON.stringify({
						model: OPENAI_CHAT_MODEL,
						stream: true,
						temperature: 0.3,
						messages: chatMessages,
					}),
				});

				if (!res.ok || !res.body) {
					throw new Error(`Chat request failed (${res.status})`);
				}

				const reader = res.body.getReader();
				const decoder = new TextDecoder('utf-8');
				let buf = '';

				while (true) {
					const { value, done } = await reader.read();
					if (done) break;
					buf += decoder.decode(value, { stream: true });

					const lines = buf.split('\n');
					buf = lines.pop() || '';
					for (const line of lines) {
						const trimmed = line.trim();
						if (!trimmed.startsWith('data:')) continue;
						const payload = trimmed.slice('data:'.length).trim();
						if (payload === '[DONE]') continue;
						let parsed;
						try { parsed = JSON.parse(payload); } catch { continue; }
						const delta = parsed?.choices?.[0]?.delta?.content;
						if (!delta) continue;
						acc += delta;
						updateStreamingBubble(assistantBubble, acc);
					}
				}
			} catch (e) {
				// Browser CORS can block direct OpenAI calls; fallback to same-origin proxy.
				usedProxy = true;
				const proxyRes = await fetch(ENDPOINTS.demoChat, {
					method: 'POST',
					headers: { 'Content-Type': 'application/json' },
					body: JSON.stringify({
						text: userText,
						history: chatMessages.filter((m) => m.role !== 'system'),
						expert_name: expert.name,
						expert_bio: expert.bio,
						expert_tags: expert.tags,
					}),
				});
				const proxyJson = await proxyRes.json();
				if (!proxyRes.ok || !proxyJson?.success) {
					throw new Error(proxyJson?.error || 'Chat proxy failed');
				}
				acc = String(proxyJson.text || '').trim();
				updateStreamingBubble(assistantBubble, acc);
			}

			const finalText = acc.trim() || '…';
			updateStreamingBubble(assistantBubble, finalText);
			chatMessages.push({ role: 'assistant', content: finalText });
			subStatusEl.textContent = usedProxy ? 'Ready (proxied)' : 'Ready';
			sendBtn.disabled = false;
		};

		// ---- Cartesia playback helpers (browser) ----
		const sanitizeForTts = (s) => {
			// Cartesia TTS: keep it simple. Remove markdown-ish formatting and collapse whitespace.
			// Important: remove "**" and "*" so it never gets spoken/causes issues.
			return String(s || '')
				.replace(/\*\*/g, '')      // bold markers
				.replace(/\*/g, '')        // stray asterisks
				.replace(/`+/g, '')        // code ticks
				.replace(/_{1,3}/g, '')    // underscores emphasis
				.replace(/#{1,6}\s*/g, '') // headings
				.replace(/\s+/g, ' ')
				.trim();
		};

		const base64ToUint8Array = (b64) => {
			const bin = atob(b64);
			const out = new Uint8Array(bin.length);
			for (let i = 0; i < bin.length; i++) out[i] = bin.charCodeAt(i);
			return out;
		};

		const createWavHeader = (dataLength, sampleRate, numChannels, bitsPerSample) => {
			const byteRate = sampleRate * numChannels * (bitsPerSample / 8);
			const blockAlign = numChannels * (bitsPerSample / 8);
			const header = new Uint8Array(44);
			const view = new DataView(header.buffer);
			view.setUint32(0, 0x46464952, true); // "RIFF"
			view.setUint32(4, 36 + dataLength, true);
			view.setUint32(8, 0x45564157, true); // "WAVE"
			view.setUint32(12, 0x20746d66, true); // "fmt "
			view.setUint32(16, 16, true);
			view.setUint16(20, 3, true); // IEEE float
			view.setUint16(22, numChannels, true);
			view.setUint32(24, sampleRate, true);
			view.setUint32(28, byteRate, true);
			view.setUint16(32, blockAlign, true);
			view.setUint16(34, bitsPerSample, true);
			view.setUint32(36, 0x61746164, true); // "data"
			view.setUint32(40, dataLength, true);
			return header;
		};

		let audioCtx = null;
		const stopCurrentTtsPlayback = () => {
			try {
				if (call?.currentAudio) {
					call.currentAudio.pause();
					call.currentAudio.currentTime = 0;
					// Drop the source to stop downloading/decoding if still in progress.
					call.currentAudio.src = '';
				}
			} catch {}
			try {
				if (call?.currentAudioResolve) call.currentAudioResolve();
			} catch {}
			try {
				if (call) {
					call.currentAudio = null;
					call.currentAudioResolve = null;
				}
			} catch {}
		};
		const playBase64PcmF32AsWav = async (base64Audio, sampleRate = 24000) => {
			// Use an <audio> element with WAV data URI (simple and reliable).
			const pcm = base64ToUint8Array(base64Audio);
			const wavHeader = createWavHeader(pcm.length, sampleRate, 1, 32);
			const wav = new Uint8Array(wavHeader.length + pcm.length);
			wav.set(wavHeader, 0);
			wav.set(pcm, wavHeader.length);

			let b64 = '';
			const chunk = 0x8000;
			for (let i = 0; i < wav.length; i += chunk) {
				b64 += String.fromCharCode.apply(null, wav.subarray(i, i + chunk));
			}
			const uri = `data:audio/wav;base64,${btoa(b64)}`;

			const a = new Audio(uri);
			a.preload = 'auto';
			await a.play();
			try {
				call.currentAudio = a;
			} catch {}
			await new Promise((resolve) => {
				try { call.currentAudioResolve = resolve; } catch {}
				a.onended = () => {
					try {
						if (call.currentAudio === a) {
							call.currentAudio = null;
							call.currentAudioResolve = null;
						}
					} catch {}
					resolve();
				};
				a.onerror = () => {
					try {
						if (call.currentAudio === a) {
							call.currentAudio = null;
							call.currentAudioResolve = null;
						}
					} catch {}
					resolve();
				};
			});
		};

		const fetchCartesiaAudio = async (text) => {
			const voiceIdToUse = (expert && expert.voice_id) ? String(expert.voice_id) : DEFAULT_CARTESIA_VOICE_ID;
			const body = new URLSearchParams({
				text,
				voice_id: voiceIdToUse,
				user_id: DEMO_USER_ID,
				pass: DEMO_PASS,
			}).toString();
			const res = await fetch(ENDPOINTS.cartesiaTts, {
				method: 'POST',
				headers: { 'Content-Type': 'application/x-www-form-urlencoded' },
				body,
				cache: 'no-store',
			});
			const data = await res.json();
			if (!data?.success || !data.audio) throw new Error(data?.error || 'Cartesia TTS failed');
			return data;
		};

		// ---- Voice call (OpenAI Realtime + Cartesia speech) ----
		let call = {
			active: false,
			pc: null,
			dc: null,
			stream: null,
			micTrack: null,
			analysisTrack: null, // clone for barge-in detection while outbound mic is muted
			bargeInEnabled: true,
			isTtsPlaying: false,
			vad: { ctx: null, raf: null, analyser: null, src: null, data: null, speechFrames: 0 },
			ignoreAssistant: false, // set true on barge-in; stops TTS enqueue while server finishes its current turn
			assistantText: '',
			speechQueue: [],
			speechWorker: false,
			ttsStreamText: '',
			ttsCursor: 0,
			stopRequested: false,
			currentAudio: null,
			currentAudioResolve: null,
		};

		const startBargeInDetector = (mediaStreamTrack) => {
			try {
				if (!mediaStreamTrack) return;
				const AudioCtx = window.AudioContext || window.webkitAudioContext;
				if (!AudioCtx) return;

				// Clean any previous detector
				stopBargeInDetector();

				const ctx = new AudioCtx();
				const stream = new MediaStream([mediaStreamTrack]);
				const src = ctx.createMediaStreamSource(stream);
				const analyser = ctx.createAnalyser();
				analyser.fftSize = 1024;
				src.connect(analyser);
				const data = new Uint8Array(analyser.fftSize);

				const THRESH = 0.03; // RMS threshold; tuned for speech vs noise
				const MIN_FRAMES = 6; // ~100ms at 60fps before triggering

				const loop = () => {
					try {
						analyser.getByteTimeDomainData(data);
						let sum = 0;
						for (let i = 0; i < data.length; i++) {
							const v = (data[i] - 128) / 128;
							sum += v * v;
						}
						const rms = Math.sqrt(sum / data.length);

						const shouldDetect = !!(call.active && call.bargeInEnabled && call.isTtsPlaying && (call.currentAudio || call.speechWorker));
						if (shouldDetect) {
							if (rms > THRESH) call.vad.speechFrames += 1;
							else call.vad.speechFrames = 0;

							if (call.vad.speechFrames >= MIN_FRAMES) {
								call.vad.speechFrames = 0;
								interruptVoicePlayback(); // stop TTS + cancel response + unmute outbound mic
							}
						} else {
							call.vad.speechFrames = 0;
						}
					} catch {
						// ignore
					}
					call.vad.raf = requestAnimationFrame(loop);
				};

				call.vad = { ctx, raf: requestAnimationFrame(loop), analyser, src, data, speechFrames: 0 };
			} catch {
				// ignore
			}
		};

		const stopBargeInDetector = () => {
			try { if (call.vad?.raf) cancelAnimationFrame(call.vad.raf); } catch {}
			try { if (call.vad?.ctx) call.vad.ctx.close(); } catch {}
			call.vad = { ctx: null, raf: null, analyser: null, src: null, data: null, speechFrames: 0 };
		};

		const findTtsBoundary = (text, startIdx) => {
			// Bigger chunks reduce "pause…pause…" cadence caused by per-chunk TTS latency.
			// We still keep an upper bound so replies start quickly and feel live.
			// Key trick: first chunk should be small (low latency), later chunks can be larger (smoothness).
			const isFirst = (startIdx || 0) === 0;
			const MIN_CHARS = isFirst ? 42 : 90;
			const MAX_CHARS = isFirst ? 220 : 340;
			const s = Math.max(0, startIdx || 0);
			if (s >= text.length) return null;

			const end = Math.min(text.length, s + MAX_CHARS);
			const windowText = text.slice(s, end);

			// Prefer sentence-ish punctuation after MIN_CHARS.
			const punctRe = /[.!?]\s+|[.!?]$|[\n\r]+/g;
			let best = -1;
			let m;
			while ((m = punctRe.exec(windowText)) !== null) {
				const idx = m.index + m[0].length;
				if (idx >= MIN_CHARS) best = idx;
			}
			if (best !== -1) return s + best;

			// For the first chunk, allow a whitespace boundary once it's "long enough" so it starts fast.
			if (isFirst) {
				const ws = windowText.lastIndexOf(' ');
				if (ws >= MIN_CHARS) return s + ws + 1;
			}

			// For later chunks, only cut at whitespace if we're already at max chunk size.
			// This avoids tiny mid-phrase chunks like "you need to" that cause awkward pauses.
			if (windowText.length >= MAX_CHARS) {
				const ws = windowText.lastIndexOf(' ');
				if (ws >= MIN_CHARS) return s + ws + 1;
				return s + MAX_CHARS;
			}

			return null;
		};

		const maybeEnqueueStreamingTts = () => {
			// Called while assistant text is streaming in.
			const text = call.ttsStreamText || '';
			let cursor = call.ttsCursor || 0;
			while (true) {
				const boundary = findTtsBoundary(text, cursor);
				if (boundary == null || boundary <= cursor) break;
				const chunk = text.slice(cursor, boundary).trim();
				cursor = boundary;
				if (chunk) enqueueSpeech(chunk);
			}
			call.ttsCursor = cursor;
		};

		const flushStreamingTts = () => {
			// Called when assistant response is done.
			const text = (call.ttsStreamText || '').slice(call.ttsCursor || 0).trim();
			if (text) enqueueSpeech(text);
			call.ttsStreamText = '';
			call.ttsCursor = 0;
		};

		const enqueueSpeech = (text) => {
			const t = sanitizeForTts(text);
			if (!t) return;
			// Prefetch TTS immediately so by the time we finish playing the previous chunk,
			// the next audio is often already ready (reduces dead-air).
			call.speechQueue.push({
				text: t,
				promise: fetchCartesiaAudio(t),
			});
			processSpeechQueue();
		};

		const interruptVoicePlayback = () => {
			// Stop playing audio immediately + drop queued speech.
			call.stopRequested = true;
			call.ignoreAssistant = true;
			call.speechQueue = [];
			stopCurrentTtsPlayback();

			// Reset streaming buffers so new speech starts clean.
			call.assistantText = '';
			call.ttsStreamText = '';
			call.ttsCursor = 0;
			call.isTtsPlaying = false;
			call.stopRequested = false;

			// Best-effort: reopen mic quickly (lets user speak right away).
			try { if (call.micTrack) call.micTrack.enabled = true; } catch {}

			// UI: go back to listening state.
			if (call.active) setVoiceStatus('Connected • speak anytime', 'active');
		};

		const processSpeechQueue = async () => {
			if (call.speechWorker) return;
			call.speechWorker = true;
			try {
				while (call.speechQueue.length) {
					if (call.stopRequested) break;
					const item = call.speechQueue.shift();
					const text = typeof item === 'string' ? item : (item?.text || '');
					setVoiceStatus('Speaking…', 'busy');
					// Prevent feedback loop: mute mic while we play TTS (echo cancellation isn't perfect).
					call.isTtsPlaying = true;
					try { if (call.micTrack) call.micTrack.enabled = false; } catch {}
					// If we already kicked off the fetch, await it; otherwise fetch now.
					const tts = (item && typeof item === 'object' && item.promise)
						? await item.promise
						: await fetchCartesiaAudio(text);
					if (call.stopRequested) break;
					await playBase64PcmF32AsWav(tts.audio, tts.sample_rate || 24000);
					if (call.stopRequested) break;
					// Small cooldown before reopening mic (helps avoid tail pickup).
					await new Promise((r) => setTimeout(r, 120));
					try { if (call.micTrack) call.micTrack.enabled = true; } catch {}
					call.isTtsPlaying = false;
				}
			} catch (e) {
				console.warn('[demo] speech queue error', e);
				setVoiceStatus('Voice error (TTS)', 'error');
			} finally {
				call.speechWorker = false;
				call.stopRequested = false;
				call.isTtsPlaying = false;
				try { if (call.micTrack) call.micTrack.enabled = true; } catch {}
				if (call.active) setVoiceStatus('Connected • speak anytime', 'active');
			}
		};

		const startVoiceCall = async () => {
			if (call.active) return;
			setVoiceStatus('Requesting microphone…', 'busy');
			subStatusEl.textContent = 'Voice connecting…';
			phoneBtn.textContent = 'Hang up';
			phoneBtn.className = 'px-3 py-2 rounded-lg bg-red-600 hover:bg-red-500 text-sm font-medium';

			try {
				const key = await getOpenAIKey();
				const stream = await navigator.mediaDevices.getUserMedia({
					audio: {
						channelCount: 1,
						echoCancellation: true,
						noiseSuppression: true,
					},
					video: false,
				});
				call.stream = stream;
				call.micTrack = stream.getAudioTracks ? (stream.getAudioTracks()[0] || null) : null;
				try {
					// Clone track so we can detect the user's speech even while the outbound track is muted during TTS.
					call.analysisTrack = call.micTrack ? call.micTrack.clone() : null;
				} catch {
					call.analysisTrack = null;
				}
				startBargeInDetector(call.analysisTrack || call.micTrack);

				const pc = new RTCPeerConnection();
				call.pc = pc;

				stream.getTracks().forEach((track) => pc.addTrack(track, stream));
				const dc = pc.createDataChannel('oai-events', { ordered: true });
				call.dc = dc;

				dc.onopen = () => {
					setVoiceStatus('Connected • speak anytime', 'active');
					subStatusEl.textContent = 'Voice connected';

					const sessionUpdate = {
						type: 'session.update',
						session: {
							modalities: ['text'],
							instructions: buildSystemPrompt(),
							input_audio_transcription: { model: 'whisper-1' },
							turn_detection: {
								type: 'server_vad',
								threshold: 0.45,
								prefix_padding_ms: 120,
								silence_duration_ms: 180,
							},
						},
					};
					dc.send(JSON.stringify(sessionUpdate));

					// Initial greeting (same idea as CallScreen)
					const createResponse = {
						type: 'response.create',
						response: {
							modalities: ['text'],
							instructions: "Do not improvise. Say exactly: Hey, what's up? Deliver it warmly, stop there—no extra words, no follow-up.",
							max_output_tokens: 48,
						},
					};
					setTimeout(() => {
						try { dc.send(JSON.stringify(createResponse)); } catch {}
					}, 200);
				};

				dc.onmessage = (event) => {
					let msg;
					try { msg = JSON.parse(event.data); } catch { return; }

					switch (msg.type) {
						case 'conversation.item.input_audio_transcription.completed': {
							const userText = (msg.transcript || '').trim();
							if (userText) {
								// User spoke: allow assistant TTS again for the next turn.
								call.ignoreAssistant = false;
								addMessage({ role: 'user', content: userText });
								subStatusEl.textContent = 'Thinking…';
							}
							break;
						}
						case 'response.text.delta':
						case 'response.output_text.delta': {
							const delta = msg.delta;
							if (!delta) break;
							if (call.ignoreAssistant) break;
							call.assistantText += delta;
							call.ttsStreamText += (sanitizeForTts(delta) + ' ');

							// Stream into the UI too (single live bubble)
							const last = messagesEl.querySelector('[data-live-assistant="1"]');
							if (last) {
								last.textContent = call.assistantText;
							} else {
								const b = addMessage({ role: 'assistant', content: call.assistantText, streaming: true });
								b.dataset.liveAssistant = '1';
							}

							// Start speaking before the full message is finished (low latency).
							maybeEnqueueStreamingTts();
							break;
						}
						case 'response.text.done':
						case 'response.output_text.done': {
							const finalText = ((msg.text ?? call.assistantText) || '').trim();
							if (!finalText) break;
							if (call.ignoreAssistant) {
								// We barged-in; ignore the tail of this assistant turn.
								call.assistantText = '';
								call.ttsStreamText = '';
								call.ttsCursor = 0;
								break;
							}

							// finalize bubble
							const last = messagesEl.querySelector('[data-live-assistant="1"]');
							if (last) {
								last.textContent = finalText;
								delete last.dataset.liveAssistant;
							} else {
								addMessage({ role: 'assistant', content: finalText });
							}

							call.assistantText = '';
							subStatusEl.textContent = 'Voice connected';

							// Speak any remaining tail that didn't hit a boundary.
							flushStreamingTts();
							break;
						}
						case 'error': {
							console.error('[demo] realtime error', msg.error);
							setVoiceStatus('Voice error (Realtime)', 'error');
							break;
						}
						default:
							break;
					}
				};

				// SDP exchange with OpenAI Realtime
				const offer = await pc.createOffer({ offerToReceiveAudio: true });
				await pc.setLocalDescription(offer);

				let answerSdp = '';
				try {
					const rtcRes = await fetch(`https://api.openai.com/v1/realtime?model=${encodeURIComponent(OPENAI_REALTIME_MODEL)}`, {
						method: 'POST',
						headers: {
							'Authorization': `Bearer ${key}`,
							'Content-Type': 'application/sdp',
						},
						body: offer.sdp,
					});
					if (!rtcRes.ok) {
						const errText = await rtcRes.text();
						throw new Error(`Realtime connect failed: ${errText}`);
					}
					answerSdp = await rtcRes.text();
				} catch (e) {
					// Browser CORS can block direct OpenAI calls; fallback to same-origin SDP proxy.
					const proxyRes = await fetch(ENDPOINTS.realtimeSdpProxy, {
						method: 'POST',
						headers: { 'Content-Type': 'application/json' },
						body: JSON.stringify({ sdp: offer.sdp, model: OPENAI_REALTIME_MODEL }),
					});
					if (!proxyRes.ok) {
						const errText = await proxyRes.text();
						throw new Error(`Realtime proxy failed: ${errText}`);
					}
					answerSdp = await proxyRes.text();
				}
				await pc.setRemoteDescription({ type: 'answer', sdp: answerSdp });

				call.active = true;
			} catch (e) {
				console.error('[demo] startVoiceCall error', e);
				setVoiceStatus('Voice call failed', 'error');
				subStatusEl.textContent = 'Ready';
				stopVoiceCall();
				alert(e?.message || 'Voice call failed');
			}
		};

		const stopVoiceCall = () => {
			try { interruptVoicePlayback(); } catch {}
			try { call.dc && call.dc.close(); } catch {}
			try { call.pc && call.pc.close(); } catch {}
			try { call.stream && call.stream.getTracks().forEach((t) => t.stop()); } catch {}
			try { call.analysisTrack && call.analysisTrack.stop(); } catch {}
			stopBargeInDetector();
			call.dc = null;
			call.pc = null;
			call.stream = null;
			call.micTrack = null;
			call.analysisTrack = null;
			call.active = false;
			call.assistantText = '';
			call.ttsStreamText = '';
			call.ttsCursor = 0;
			call.speechQueue = [];
			call.speechWorker = false;
			call.stopRequested = false;
			call.isTtsPlaying = false;
			call.ignoreAssistant = false;
			call.currentAudio = null;
			call.currentAudioResolve = null;

			setVoiceStatus('Idle', 'idle');
			subStatusEl.textContent = 'Ready';
			phoneBtn.textContent = 'Phone call';
			phoneBtn.className = 'px-3 py-2 rounded-lg bg-emerald-600 hover:bg-emerald-500 text-sm font-medium';
		};

		// ---- Video call (LiveAvatar FULL mode: HeyGen handles ASR/LLM/TTS + video) ----
		let videoCall = {
			active: false,
			sessionToken: null,
			sessionId: null,
			room: null,
			localAudioTrack: null,
			mediaStream: null,
			lastTranscript: '',
			micMeter: { ctx: null, raf: null, analyser: null, src: null },
		};

		const setVideoStatus = (text, debug = '') => {
			videoStatusEl.textContent = text;
			if (debug) videoDebugEl.textContent = debug;
		};

		const openVideoModal = () => {
			videoModalEl.classList.remove('hidden');
			videoPlaceholderEl.classList.remove('hidden');
			heygenVideoEl.srcObject = null;
			setVideoStatus('Connecting…', 'Starting session…');
		};

		const closeVideoModal = () => {
			videoModalEl.classList.add('hidden');
		};

		const getLiveAvatarSession = async () => {
			const payload = {
				avatar_id: (expert && expert.avatar_id) ? String(expert.avatar_id) : '',
				persona_prompt: buildSystemPrompt(),
			};
			const res = await fetch(ENDPOINTS.liveavatarCreate, {
				method: 'POST',
				headers: { 'Content-Type': 'application/json' },
				body: JSON.stringify(payload),
			});
			const data = await res.json().catch(() => ({}));
			if (!res.ok || !data?.success || !data.session_token) {
				throw new Error(data?.error || 'Failed to create LiveAvatar session token');
			}
			return data; // {session_token, session_id}
		};

		const startMicMeter = (mediaStreamTrack) => {
			try {
				if (!mediaStreamTrack) return;
				const AudioCtx = window.AudioContext || window.webkitAudioContext;
				if (!AudioCtx) return;
				const ctx = new AudioCtx();
				const stream = new MediaStream([mediaStreamTrack]);
				const src = ctx.createMediaStreamSource(stream);
				const analyser = ctx.createAnalyser();
				analyser.fftSize = 1024;
				src.connect(analyser);
				const data = new Uint8Array(analyser.fftSize);
				const loop = () => {
					analyser.getByteTimeDomainData(data);
					let sum = 0;
					for (let i = 0; i < data.length; i++) {
						const v = (data[i] - 128) / 128;
						sum += v * v;
					}
					const rms = Math.sqrt(sum / data.length);
					const pct = Math.max(0, Math.min(100, Math.round(rms * 220)));
					if (videoMicBarEl) videoMicBarEl.style.width = `${pct}%`;
					if (videoMicTextEl) videoMicTextEl.textContent = pct ? `${pct}%` : '—';
					videoCall.micMeter.raf = requestAnimationFrame(loop);
				};
				videoCall.micMeter = { ctx, raf: requestAnimationFrame(loop), analyser, src };
			} catch {
				// ignore
			}
		};

		const stopMicMeter = () => {
			try {
				if (videoCall.micMeter?.raf) cancelAnimationFrame(videoCall.micMeter.raf);
			} catch {}
			try {
				if (videoCall.micMeter?.ctx) videoCall.micMeter.ctx.close();
			} catch {}
			videoCall.micMeter = { ctx: null, raf: null, analyser: null, src: null };
			if (videoMicBarEl) videoMicBarEl.style.width = '0%';
			if (videoMicTextEl) videoMicTextEl.textContent = '—';
		};

		const sendAgentControl = async (room, eventName, data = null) => {
			if (!room) return;
			const payloads = [
				{ event: eventName, data: data || undefined },
				{ type: eventName, data: data || undefined },
			];
			const enc = new TextEncoder();
			for (const p of payloads) {
				try {
					// LiveAvatar FULL mode command topic: agent-control
					// eslint-disable-next-line no-await-in-loop
					await room.localParticipant.publishData(enc.encode(JSON.stringify(p)), { topic: 'agent-control', reliable: true });
					return;
				} catch {
					// try alternate key shape
				}
			}
		};

		const startLiveAvatarRoom = async () => {
			if (!window.__LIVEKIT__) throw new Error('LiveKit client failed to load. Check network / CSP.');

			// 1) Create LiveAvatar session token (short-lived)
			const sess = await getLiveAvatarSession();
			videoCall.sessionToken = sess.session_token;
			videoCall.sessionId = sess.session_id || null;

			// 2) Start session -> get LiveKit room url + room token
			const startRes = await fetch(ENDPOINTS.liveavatarStart, {
				method: 'POST',
				headers: { 'Content-Type': 'application/json' },
				body: JSON.stringify({ session_token: videoCall.sessionToken, session_id: videoCall.sessionId }),
			});
			const startJson = await startRes.json().catch(() => ({}));
			if (!startRes.ok || !startJson?.success) {
				const details = startJson?.details ? ` | ${String(startJson.details).slice(0, 200)}` : '';
				throw new Error((startJson?.error || 'LiveAvatar start failed') + details);
			}

			const roomUrl = startJson.room_url;
			const roomToken = startJson.room_token;

			// 3) Join LiveKit room and render tracks into the <video>
			const LK = window.__LIVEKIT__;
			const room = new LK.Room({ adaptiveStream: true, dynacast: true });
			videoCall.room = room;

			videoCall.mediaStream = new MediaStream();
			heygenVideoEl.srcObject = videoCall.mediaStream;

			const addTrack = (track) => {
				try {
					const mst = track?.mediaStreamTrack;
					if (mst) videoCall.mediaStream.addTrack(mst);
				} catch {}
			};

			room.on(LK.RoomEvent.TrackSubscribed, (track) => {
				addTrack(track);
				videoPlaceholderEl.classList.add('hidden');
			});
			room.on(LK.RoomEvent.DataReceived, (payload, participant, kind, topic) => {
				try {
					const msg = new TextDecoder().decode(payload || new Uint8Array());
					const j = JSON.parse(msg);
					const evt = j?.event || j?.type || j?.name || null;
					const data = j?.data || j?.payload || null;

					// Some SDKs don't surface the topic reliably; treat any recognized event as a server event.
					if (evt) {
						const tpc = topic || 'data';
						// High-signal: show last event in debug
						setVideoStatus(videoStatusEl.textContent || 'Connected', `${tpc}:${evt}`);
						if (evt === 'user.speak_started') setVideoStatus('Listening…', 'user.speak_started');
						if (evt === 'user.speak_ended') setVideoStatus('Processing…', 'user.speak_ended');
						if (evt === 'user.transcription_started') setVideoStatus('Transcribing…', 'user.transcription_started');
						if (evt === 'user.transcription_ended') {
							const t = data?.text || j?.text || '';
							if (t) {
								videoCall.lastTranscript = t;
								if (videoTranscriptEl) videoTranscriptEl.textContent = t;
							}
							setVideoStatus('Thinking…', 'user.transcription_ended');
						}
						if (evt === 'avatar.transcription_ended') {
							const t = data?.text || j?.text || '';
							if (t) setVideoStatus('Speaking…', `avatar.transcription_ended: ${t.slice(0, 60)}`);
						}
						if (evt === 'avatar.speak_started') setVideoStatus('Speaking…', 'avatar.speak_started');
						if (evt === 'avatar.speak_ended') setVideoStatus('Connected', 'avatar.speak_ended');
					}
				} catch {
					// ignore non-json data frames
				}
			});
			room.on(LK.RoomEvent.Disconnected, () => {
				setVideoStatus('Disconnected', 'LiveKit room disconnected');
			});

			await room.connect(roomUrl, roomToken);

			// 4) Publish microphone so FULL mode can hear the user
			let localAudio = null;
			try {
				localAudio = await LK.createLocalAudioTrack({
					echoCancellation: true,
					noiseSuppression: true,
				});
			} catch (e) {
				// Most common: NotAllowedError when mic permission is denied.
				const msg = (e && typeof e === 'object' && ('name' in e || 'message' in e))
					? `${e.name || 'Error'}: ${e.message || ''}`.trim()
					: String(e);
				throw new Error(`Microphone failed (${msg}). Allow mic permission for this site.`);
			}
			videoCall.localAudioTrack = localAudio;
			await room.localParticipant.publishTrack(localAudio);
			startMicMeter(localAudio?.mediaStreamTrack || null);

			videoPlaceholderEl.classList.add('hidden');
			setVideoStatus('Connected', 'Mic published. Starting listening…');
			if (videoTranscriptEl) videoTranscriptEl.textContent = '—';

			// 5) Put avatar into listening state (LiveAvatar FULL mode command)
			await sendAgentControl(room, 'avatar.start_listening');
			setVideoStatus('Listening…', 'avatar.start_listening');
		};

		const startVideoCall = async () => {
			if (videoCall.active) return;
			// Avoid running phone call and video call at the same time.
			if (call.active) stopVoiceCall();
			openVideoModal();
			videoBtn.textContent = 'Video hang up';
			videoBtn.className = 'px-3 py-2 rounded-lg bg-red-600 hover:bg-red-500 text-sm font-medium';

			try {
				setVideoStatus('Connecting…', 'Starting LiveAvatar FULL mode session…');
				await startLiveAvatarRoom();

				videoCall.active = true;
				videoPlaceholderEl.classList.add('hidden');
			} catch (e) {
				const msg = (e && typeof e === 'object' && ('name' in e || 'message' in e))
					? `${e.name || 'Error'}: ${e.message || ''}`.trim()
					: (e?.message || String(e));
				console.error('[demo] startVideoCall error', e);
				setVideoStatus('Error', msg || 'Video call failed');
				stopVideoCall();
				alert(msg || 'Video call failed');
			}
		};

		const stopVideoCall = () => {
			stopMicMeter();
			try {
				if (videoCall.localAudioTrack) {
					videoCall.localAudioTrack.stop();
				}
			} catch {}
			videoCall.localAudioTrack = null;

			try {
				if (videoCall.room) videoCall.room.disconnect();
			} catch {}
			videoCall.room = null;

			// Best-effort: stop the LiveAvatar session to release concurrency/credits.
			try {
				if (videoCall.sessionToken || videoCall.sessionId) {
					fetch(ENDPOINTS.liveavatarStop, {
						method: 'POST',
						headers: { 'Content-Type': 'application/json' },
						body: JSON.stringify({ session_token: videoCall.sessionToken, session_id: videoCall.sessionId }),
					}).catch(() => {});
				}
			} catch {}

			videoCall.active = false;
			videoCall.sessionToken = null;
			videoCall.sessionId = null;
			heygenVideoEl.srcObject = null;
			videoPlaceholderEl.classList.remove('hidden');
			setVideoStatus('Idle', '—');

			// UI reset
			videoBtn.textContent = 'Video call';
			videoBtn.className = 'px-3 py-2 rounded-lg ring-1 ring-white/10 hover:bg-white/10 text-sm';
		};

		// ---- Wire up UI ----
		inputEl.addEventListener('input', autosizeTextarea);
		window.addEventListener('load', autosizeTextarea);

		inputEl.addEventListener('keydown', (e) => {
			if (e.key === 'Enter' && !e.shiftKey) {
				e.preventDefault();
				el('composer').requestSubmit();
			}
		});

		el('composer').addEventListener('submit', async (e) => {
			e.preventDefault();
			const text = (inputEl.value || '').trim();
			if (!text) return;
			inputEl.value = '';
			autosizeTextarea();

			addMessage({ role: 'user', content: text });
			try {
				// If video call is active, route typed text into the LiveAvatar FULL-mode LLM (so avatar speaks).
				if (videoCall.active && videoCall.room) {
					await sendAgentControl(videoCall.room, 'avatar.speak_response', { text });
					return;
				}
				await streamChatCompletion(text);
			} catch (err) {
				console.error('[demo] chat error', err);
				subStatusEl.textContent = 'Error';
				addMessage({ role: 'assistant', content: 'Sorry — chat failed. Check console + backend OPENAI_API_KEY.' });
				sendBtn.disabled = false;
			} finally {
				subStatusEl.textContent = 'Ready';
			}
		});

		videoBtn.addEventListener('click', async () => {
			if (videoCall.active) {
				stopVideoCall();
				closeVideoModal();
				return;
			}
			await startVideoCall();
		});

		videoHangupBtn.addEventListener('click', () => {
			stopVideoCall();
			closeVideoModal();
		});

		videoCloseBtn.addEventListener('click', () => {
			// Close = hang up (prevents burning credits/concurrency by accident)
			stopVideoCall();
			closeVideoModal();
		});

		// Allow user gesture to start video autoplay if needed
		videoModalEl.addEventListener('click', () => {
			try { heygenVideoEl.play(); } catch {}
		});

		// If the user navigates away, ensure we stop the streaming session (avoids burning credits).
		window.addEventListener('beforeunload', () => {
			try {
				if (videoCall.active) stopVideoCall();
			} catch {}
			try {
				if (call.active) stopVoiceCall();
			} catch {}
		});

		phoneBtn.addEventListener('click', async () => {
			// Avoid running phone call and video call at the same time.
			if (videoCall.active) {
				stopVideoCall();
				closeVideoModal();
			}
			if (call.active) {
				stopVoiceCall();
				return;
			}
			await startVoiceCall();
		});

		// Barge-in shortcut: Space interrupts TTS instantly while on a phone call.
		window.addEventListener('keydown', (e) => {
			if (!call.active) return;
			if (e.code !== 'Space') return;
			// Avoid interfering with typing in the composer.
			if (document.activeElement === inputEl) return;
			if (!call.speechWorker && !call.currentAudio) return;
			e.preventDefault();
			interruptVoicePlayback();
		});

		// Boot
		setVoiceStatus('Idle', 'idle');
		renderExpert();
		loadExpertFromBackend();
	</script>
</body>
</html>

















